{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dabd8203-6e91-42d3-bde9-8e3a11638879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# pytorch models\n",
    "import pytorch_lightning as L\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "# dataset imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scikit Learn\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c95e7d-909d-431b-9558-9d3066aa82ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaion of dataset properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b950dd6-a6c2-4f93-81c7-1969c18429f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset.\n",
    "class BRISCDataset(Dataset):\n",
    "    \"\"\"This class is a version of Dataset which implements the computation in\n",
    "    the GPU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"This constructor intialize the file_path and mode attributes. Also here we\n",
    "        load the dataset to the cuda device.\n",
    "        Arguments:\n",
    "            file_path   The path of the file h5py.\n",
    "            mode        The modality for accessing the datset correcty.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preload dataset in GPU.\n",
    "        device = \"cuda\"\n",
    "\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"This function return the len of the dataset.\"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"This function returns the item at the given index.\n",
    "        Arguments:\n",
    "            idx   The data index.\n",
    "        \"\"\"\n",
    "        return self.dataset[idx]\n",
    "\n",
    "\n",
    "class BRISCDataModule(L.LightningDataModule):\n",
    "      \"\"\"This class is a version of the LightningDataModule, after initialization\n",
    "    it allows to set the dataset and then train dataloader.\n",
    "    Attributes:\n",
    "        data_dir      The directory where we have the data.\n",
    "        batch_size    The dimension of the file.\n",
    "        preload_gpu   This is a boolean variable that tells whether we should tun on GPU.\n",
    "        nw            This is the number of workers that load the dataset in parallel.\n",
    "        dataset       This variable is the dataset itself (can be of 2 types).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 0,\n",
    "        preload_gpu: bool = False,\n",
    "    ):\n",
    "        \"\"\"This constructor initialize the main class attributes.\"\"\"\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.preload_gpu = preload_gpu\n",
    "        self.nw = num_workers\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        \"\"\"This function set the dataset, here we check if we have to train on the gpu\n",
    "        or not and we decide which dataset to use depending on this.\n",
    "        \"\"\"\n",
    "        if self.preload_gpu:\n",
    "            self.dataset = BRISCDataset(self.data_dir)\n",
    "        else:\n",
    "            self.dataset = BRISCDataset(self.data_dir)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"This function is used to train the class instance on the chosen dataset.\"\"\"\n",
    "        if self.preload_gpu:\n",
    "            dataloader = DataLoader(\n",
    "                self.dataset, batch_size=self.batch_size, shuffle=True\n",
    "            )\n",
    "        else:\n",
    "            dataloader = DataLoader(\n",
    "                self.dataset,\n",
    "                num_workers=self.nw,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b3c2e-34f6-418d-8e0b-c7be8be80cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708f1103-7a5f-43f8-b9fe-93706a6a929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and fine tuning.\n",
    "class ResNetFineTuner(pl.LightningModule):\n",
    "    def __init__(self, num_classes=10, lr=1e-3, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Load pretrained ResNet50\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        if freeze_backbone:\n",
    "            for param in backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Replace classifier head\n",
    "        num_ftrs = backbone.fc.in_features\n",
    "        backbone.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        self.model = backbone\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        val_loss = self.criterion(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean()\n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc490b7-1a4e-4f86-afd2-2c14e7767992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation and Test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304aa87-87f6-44b8-b983-a16064513453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing statistics (seaborn)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL & ML)",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
